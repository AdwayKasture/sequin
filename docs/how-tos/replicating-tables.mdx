---
title: "How to replicate tables with Sequin"
sidebarTitle: "Replicate tables"
description: "Stream and replicate tables between databases"
---

Sequin makes it easy to replicate tables between databases. This guide shows you how to:

- **Stream an existing table**: Copy all rows from a source table
- **Keep tables in sync**: Maintain an up-to-date copy as the source changes
- **Transform data**: Modify the data before writing to the destination

## Prerequisites

This guide assumes you already have Sequin installed and have databases connected. If you don't, see the [quickstart guide](/quickstart).

## Overview

This guide shows you how to replicate a table from one database to another. For example, if you have a `users` table in your primary database and want to maintain a copy in your analytics database.

You'll perform these steps:

1. **[Create a stream](#create-a-stream)**: The stream will contain rows from your source table
2. **[Setup the destination table](#setup-the-destination-table)**: Create the table you want to replicate to
3. Either **[Create a Consumer Group](#create-a-consumer-group)** or **[Create a Webhook Subscription](#consume-with-webhooks)**: Use either tool to process rows
4. **[Upsert rows to the destination](#upserting-into-the-destination)**: Write the upsert logic in your worker code

### Start locally

You can start by following all the steps below in your local environment. At the end of this guide are instructions for [deploying to production](#deploying-to-production).

## Create a stream

First, create a stream of your source table. Streams connect your table to Sequin's Consumer Groups or Webhook Subscriptions:

<Steps>
  <Step title="Navigate to Streams">
    In the Sequin web console, click on the "Streams" tab. Click "Create Stream".
  </Step>

  <Step title="Select your source table">
    Select the table you want to replicate.

    Under "Sort and start", choose a good sort column. `updated_at` is a good choice for most tables, while `created_at` is fine for append-only tables.

    <Info>
      The sort column will be used to order rows in your stream whenever you're playing historical data. [Read more about sort columns](/how-sequin-works#streams).
    </Info>

    <Frame>
      <img style={{maxWidth: '500px'}} src="/images/how-tos/replicating-tables/create-stream.png" alt="Stream configuration" />
    </Frame>
  </Step>

  <Step title="Create the stream">
    Click "Create Stream" to make your table available for replication.
  </Step>
</Steps>

## Setup the destination table

Next, create the table you want to replicate to. This can be in the same database as the source table, or a different database.

The most important consideration is that you'll want your [upsert logic](#upserting-into-the-destination) to be idempotent. That means you'll use an `insert into ... on conflict` statement.

We recommend adding a column to your destination table that corresponds to the primary key of your source table. For example, if your source table has a `product_id` primary key:

```sql
create table replicated_products (
  product_id uuid primary key,  -- matches source table PK
  name text,
  price decimal,
  created_at timestamp
);
```

## Create a Consumer Group

You can use a Consumer Group to pull rows from your stream and write them to your destination table:

<Steps>
  <Step title="Create a Consumer Group">
    Click on the "Consumer Groups" tab in the Sequin web console. Click "Create Consumer Group".
  </Step>

  <Step title="Configure the source">
    Under "Source", select the stream you created.

    For "Records to process", you can optionally add filters to sync a subset of your source table to the destination. You can filter for `subscriptions` with a certain `status` or orders over a certain `total_amount`:

    <Frame>
      <img style={{maxWidth: '500px'}} src="/images/how-tos/replicating-tables/records-to-process.png" alt="Records to process" />
    </Frame>

    You can always change these filters later and [re-backfill the destination(#re-backfill-the-destination).

    For "Where should the consumer group start?", you can indicate how far back in time you want to sync rows from the source table. Rows are ordered by the sort column you chose when you created the stream. So if you chose a field like `updated_at`, you can choose to sync rows from the beginning of the table, from the last few months, etc.

    If you want the destination to contain all rows from the source, choose "At the beginning of the table." If you want to sync only new rows, choose "At the end of the table." Otherwise, choose a specific point in time according to your needs.
  </Step>

  <Step title="Configure message grouping">
    Under "Message grouping", leave the default option selected.

    <Info>
      Sequin will group messages by primary key. This means that if a consumer pull or webhook push is outstanding for a row, Sequin will not deliver that row until the outstanding message is processed. This ensures there's no risk of a race condition as you write rows to the destination.
    </Info>
  </Step>

  <Step title="Create and test the Consumer Group">
    Click "Create Consumer Group".

    To make sure things are working end-to-end, you can copy the example `curl` request on the Consumer Group's page and run it. The endpoint should return a batch of rows from your source table.
  </Step>
</Steps>

### Setup your application

In your application, you'll setup a worker that pulls messages from the Consumer Group and upserts them into your destination table.

For "Batch size", if you think it will be difficult for you to [batch upsert rows](#upserting-into-the-destination) with your ORM or database adapter, leave at the default value of `1`. Otherwise, we recommend you set it to a batch size of at least 100. This will improve overall throughput. (Depending on your hardware and use case, you'll see diminishing returns upserting batch sizes greater than 1000 into Postgres.)

Here's a basic example using Sequin's Go SDK:

```go
func main() {
    client := sequin.NewClient(&sequin.ClientOptions{
        Token: os.Getenv("SEQUIN_TOKEN"),
    })

    processor, err := sequin.NewProcessor(
        client,
        "my-consumer-group",
        processRows,
        sequin.ProcessorOptions{
            MaxBatchSize: 100,  // Process up to 100 rows at once
            MaxConcurrent: 10,
        },
    )
    if err != nil {
        log.Fatalf("Failed to create processor: %v", err)
    }

    if err := processor.Start(); err != nil {
        log.Fatalf("Processor failed: %v", err)
    }
}

func processRows(ctx context.Context, msgs []sequin.Message) error {
    // See below for the logic to upsert rows into your destination
    res, err := upsertRows(ctx, msgs)
    if err != nil {
        return err
    }

    log.Printf("Successfully processed %d rows", len(msgs))
    return nil
}
```

## Consume with webhooks

Instead of pulling rows with a Consumer Group, you can have Sequin push them to a webhook endpoint. Each webhook payload will contain one row. Rows are processed exactly once per subscription.

<Steps>
  <Step title="Navigate to Webhook Subscriptions">
    In the Sequin web console, under destinations, navigate to the Webhook Subscriptions tab and click "Create Webhook Subscription".
  </Step>

  <Step title="Configure the source">
    Under Source, select your stream.
  </Step>

  <Step title="Configure the webhook">
    Under "Webhook configuration", enter your endpoint URL. This is where Sequin will send the webhook payloads.

    For "Batch size", if you think it will be difficult for you to [batch upsert rows](#upserting-into-the-destination) with your ORM or database adapter, leave at the default value of `1`. Otherwise, we recommend you set it to a batch size of at least 100. This will improve overall throughput. (Depending on your hardware and use case, you'll see diminishing returns upserting batch sizes greater than 1000 into Postgres.)

    Remember, you'll want to insert all rows you receive in a webhook payload in a single statement.
  </Step>

  <Step title="Configure message grouping">
    Under "Message grouping", leave the default option selected to ensure rows are processed in order.
  </Step>

  <Step title="Create the subscription">
    Click "Create Webhook Subscription".
  </Step>
</Steps>

### Handle webhooks in your application

Your webhook endpoint will receive POST requests with [row payloads](/consume/webhooks).

Here's an example of handling webhooks with Node.js and Express:

```javascript
app.post('/webhooks/replicate-users', async (req, res) => {
  try {
    const row = req.body.record;
    
    // Upsert the row into your destination table
    await upsertRow(row);

    // Return 200 to acknowledge successful processing
    res.sendStatus(200);
  } catch (error) {
    console.error('Error processing webhook:', error);
    // Return 500 to trigger a retry
    res.sendStatus(500);
  }
});
```

If your endpoint returns a non-200 status code or fails to respond within the configured timeout, Sequin will retry the webhook indefinitely with an exponential backoff.

## Upserting into the destination

You'll upsert rows into your destination table using an `insert into ... on conflict` statement. This ensures idempotency - the same row can only be written once.

The conflict key will be the primary key column that matches your source table.

The process will be:

1. Map rows to your destination schema
2. Perform the upsert, ideally in a single statement

### Map rows to your schema

You'll need to map the source table's schema to your destination schema. This might involve:

- Renaming columns
- Transforming values
- Adding/removing columns
- Type conversions

Sequin sends rows to your consumer in JSON. JSON's types are not as rich as Postgres' types. So you may need to cast the types appropriately:

- **Timestamps/dates**: Transported in JSON as strings, need casting to `timestamp` or `date`
- **UUIDs**: Transported in JSON as strings, need casting to `uuid`
- **Numeric types**: Transported in JSON as numbers, might need explicit casting to `decimal`, `bigint`, etc.

### Batch upserts

Ideally, you batch your upserts. For example, if you're processing 100 rows, you'll upsert all 100 in a single statement. This is far more efficient than upserting individual rows.

If you're using an ORM, consult your ORM's documentation for how to perform batched upserts. For example, here's what upserts look like with `ActiveRecord`:

```ruby
class User < ApplicationRecord
  def self.batch_upsert_users(users_data)
    users = users_data.map { |data| new(data) }
    
    User.upsert_all(
      users.map(&:attributes),
      unique_by: :user_id,
      update_only: User.column_names - ['id', 'created_at'],
      returning: :user_id
    )
  end
end
```

In raw SQL, that looks something like:

```sql
insert into replicated_users (
  user_id,
  name,
  email,
  created_at
) values 
  ($1, $2, $3, $4),
  ($5, $6, $7, $8),
  ($9, $10, $11, $12)
on conflict (user_id) do update set
  name = excluded.name,
  email = excluded.email,
  created_at = excluded.created_at;
```

## Deploying to production

Once you've tested your replication pipeline locally, you can deploy it to production.

You can use the Sequin CLI to export your local configuration and apply it to your production environment:

<Steps>
  <Step title="Perform migrations">
    Ensure your production destination database has the tables you want to replicate to.
    
    We recommend deploying and migrating before proceeding.
  </Step>

  <Step title="Set your Sequin token">
    If you're using a Consumer Group, ensure your Sequin token is configured in your production environment.
  </Step>

  <Step title="Export your configuration">
    After [setting up the Sequin CLI](/cli), export your local configuration:

    ```bash
    sequin config export --context=local
    ```

    This creates a `sequin.yaml` file you can commit to your repository.

    <Info>
      Consider having different YAML files for different environments. For example, `sequin.staging.yaml` and `sequin.production.yaml`.

      Database names often differ between environments (e.g. `my_db_dev` vs `my_db_prod`).
    </Info>
  </Step>

  <Step title="Apply to production">
    Apply your configuration to production:

    ```bash
    sequin config apply sequin.yaml --context=production
    ```
  </Step>
</Steps>

## Re-backfilling
